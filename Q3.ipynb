{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cba2d5",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3207876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import heapq\n",
    "import operator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d58ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedDict, SortedList, SortedSet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4642501",
   "metadata": {},
   "source": [
    "# Read From Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55826de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(directory):\n",
    "    '''\n",
    "    Parameters:\n",
    "        directory: type(string)\n",
    "        \n",
    "    returns: list of all files in directory with the full path of file\n",
    "    '''\n",
    "    \n",
    "    list_of_files = []\n",
    "    \n",
    "    for file_path in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, file_path)\n",
    "        if os.path.isfile(full_path):\n",
    "            list_of_files.append(full_path)\n",
    "    \n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5655daec",
   "metadata": {},
   "source": [
    "# Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd821873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(data):\n",
    "    '''\n",
    "    Parameters:\n",
    "        data: type(string)\n",
    "    \n",
    "    returns: lowercase of data\n",
    "    '''\n",
    "    \n",
    "    return data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc555d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_word_tokenize(corpus):\n",
    "    '''\n",
    "    Parameters:\n",
    "        corpus: type(string)\n",
    "    \n",
    "    returns word-level tokenization of corpus\n",
    "    '''\n",
    "    \n",
    "    return word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4849df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_tokens(tokens, stopwords_set):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "        stopwords_set: type(set)\n",
    "    \n",
    "    returns: tokens without stopwords\n",
    "    '''\n",
    "    tokens_sans_stopwords = [x for x in tokens if x not in stopwords_set]\n",
    "    \n",
    "    return tokens_sans_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfe92273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_from_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: tokens without punctuation\n",
    "    '''\n",
    "    tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in tokens]\n",
    "    \n",
    "    return tokens_sans_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586f5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_blank_space_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: tokens without blank tokens\n",
    "    '''\n",
    "    tokens_sans_blank_space = [x for x in tokens if x!='']\n",
    "    \n",
    "    return tokens_sans_blank_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "683f40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    retruns: tokens after stemming\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_tokens = [stemmer.stem(x) for x in tokens]\n",
    "    return stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95b966fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: returns tokens after lemmatization\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatize_tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "    #unique_lemmatize_tokens = list(dict.fromkeys(lemmatize_tokens))\n",
    "    \n",
    "    return lemmatize_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36371bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, stopwords_set):\n",
    "    # Convert the text to lower case\n",
    "    lowercase_corpus = lowercase(corpus)\n",
    "    #print(len(lowercase_corpus))\n",
    "    \n",
    "    # Perform word tokenization (word_tokenize also takes care of whitespace)\n",
    "    word_tokens = perform_word_tokenize(lowercase_corpus)\n",
    "    #print(len(word_tokens))\n",
    "    \n",
    "    # Remove stopwords from tokens\n",
    "    word_tokens_sans_stopwords = remove_stopwords_from_tokens(word_tokens, stopwords_set)\n",
    "    #print(len(word_tokens_sans_stopwords))\n",
    "    \n",
    "    # Remove punctuation marks from tokens\n",
    "    word_tokens_sans_punctuation = remove_punctuation_from_tokens(word_tokens_sans_stopwords)\n",
    "    #print(len(word_tokens_sans_punctuation))\n",
    "    \n",
    "    # Remove blank space tokens\n",
    "    word_tokens_sans_blank_tokens = remove_blank_space_tokens(word_tokens_sans_punctuation)\n",
    "    #print(len(word_tokens_sans_blank_tokens))\n",
    "    \n",
    "    # Stem tokens\n",
    "    word_tokens_final = stemming(word_tokens_sans_blank_tokens)\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    #word_tokens_final = lemmatize_tokens(word_tokens_sans_blank_tokens)\n",
    "    #print(len(word_tokens_final))\n",
    "    \n",
    "    return word_tokens_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df8bf1",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e87e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_file_dictionary(list_of_files):\n",
    "#     '''\n",
    "#     Paramteres:\n",
    "#         list_of_files: type(string)\n",
    "    \n",
    "#     returns: file_dictionary with integer key and path_of_file as value\n",
    "#     '''\n",
    "#     file_dictionary = {}\n",
    "#     for i in range(len(list_of_files)):\n",
    "#         file_dictionary[i] = list_of_files[i]\n",
    "    \n",
    "#     return file_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f6802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_file_list(list_of_files):\n",
    "#     '''\n",
    "#     Paramteres:\n",
    "#         list_of_files: type(string)\n",
    "    \n",
    "#     returns: file_list with path_of_file as values\n",
    "#     '''\n",
    "#     file_dictionary = []\n",
    "#     for i in range(len(list_of_files)):\n",
    "#         file_dictionary[i] = list_of_files[i]\n",
    "    \n",
    "#     return file_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2351161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_function(train_frac, list_of_files_class_c):\n",
    "    '''\n",
    "    Parameters:\n",
    "        train_frac: type(float)\n",
    "        \n",
    "    returns: train test list in ratio train_frac:1-train_frac\n",
    "    '''\n",
    "    random.shuffle(list_of_files_class_c)\n",
    "    train_size = int(train_frac*len(list_of_files_class_c))\n",
    "    train_list_class_c = list_of_files_class_c[:train_size]\n",
    "    test_list_class_c = list_of_files_class_c[train_size:]\n",
    "    \n",
    "    return train_list_class_c, test_list_class_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6370a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(list_of_files, num_of_classes, stopwords_set):\n",
    "    '''\n",
    "    Parameters:\n",
    "        list_of_files: type(list)\n",
    "        num_of_classes: type(int)\n",
    "        stopwords_set: type(set)\n",
    "    \n",
    "    returns: list of tokens obtained by preprocessing documents in all classes\n",
    "    '''\n",
    "    preprocessed_list_of_docs_tokens = []\n",
    "    for c in tqdm(range(num_of_classes)):\n",
    "        doc_tokens_class_c = []\n",
    "        for doc_path in list_of_files[c]:\n",
    "            file = open(doc_path, 'r', encoding='utf-8', errors='ignore')\n",
    "            file_corpus = file.read()\n",
    "            file.close()\n",
    "            doc_tokens = preprocess(file_corpus, stopwords_set)\n",
    "            doc_tokens_class_c.append(doc_tokens)\n",
    "        preprocessed_list_of_docs_tokens.append(doc_tokens_class_c)\n",
    "    \n",
    "    pi_file = open('Q3_pkl_file', 'wb')\n",
    "    pickle.dump(preprocessed_list_of_docs_tokens, pi_file)\n",
    "    pi_file.close()\n",
    "    \n",
    "    return preprocessed_list_of_docs_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2413208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_icf_list(train_test_lists, num_of_classes):\n",
    "    tf_list_of_dict = []\n",
    "    for c in range(num_of_classes):\n",
    "        tf_list_of_dict.append({})\n",
    "    cf_dict = {}\n",
    "    icf_dict = {}\n",
    "    for c in range(num_of_classes):\n",
    "        for doc_terms in train_test_lists[c][0]:\n",
    "            for term in doc_terms:\n",
    "                if(term in tf_list_of_dict[c]):\n",
    "                    tf_list_of_dict[c][term]+=1\n",
    "                else:\n",
    "                    tf_list_of_dict[c][term]=1\n",
    "    \n",
    "    for tf_dict_class_c in tf_list_of_dict:\n",
    "        for term in tf_dict_class_c:\n",
    "            if(term in cf_dict):\n",
    "                cf_dict[term]+=1\n",
    "            else:\n",
    "                cf_dict[term]=1\n",
    "    \n",
    "    for term in cf_dict:\n",
    "        icf_dict[term] = math.log2(num_of_classes/cf_dict[term])\n",
    "    \n",
    "    tf_icf_list_of_dict = []\n",
    "    for c in range(num_of_classes):\n",
    "        tf_icf_list_of_dict.append({})\n",
    "    for c in range(num_of_classes):\n",
    "        for term in tf_list_of_dict[c]:\n",
    "            tf_icf_list_of_dict[c][term] = tf_list_of_dict[c][term]*icf_dict[term]\n",
    "    \n",
    "    return tf_list_of_dict, icf_dict, tf_icf_list_of_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcbe5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q3(train_test_lists, num_of_classes, top_k):\n",
    "    # tf-icf\n",
    "    tf_list_of_dict, icf_dict, tf_icf_list_of_dict = get_tf_icf_list(train_test_lists, num_of_classes)\n",
    "    list_of_vocab_class_c = []\n",
    "    for c in range(num_of_classes):\n",
    "        vocab_class_c_temp = heapq.nlargest(top_k, tf_icf_list_of_dict[c].items(), key=operator.itemgetter(1))\n",
    "        vocab_class_c = {}\n",
    "        for tup in vocab_class_c_temp:\n",
    "            vocab_class_c[tup[0]]=tup[1]\n",
    "        list_of_vocab_class_c.append(vocab_class_c)\n",
    "    \n",
    "    # global vocabulary\n",
    "    global_vocab = {}\n",
    "    for c in range(num_of_classes):\n",
    "        global_vocab = global_vocab | list_of_vocab_class_c[c].keys()\n",
    "    \n",
    "    # calculate priors\n",
    "    num_docs_class_c = []\n",
    "    total_docs = 0\n",
    "    for c in range(num_of_classes):\n",
    "        total_docs+=len(train_test_lists[c][0])\n",
    "        num_docs_class_c.append(len(train_test_lists[c][0]))\n",
    "    prior_class_c = [x/total_docs for x in num_docs_class_c]\n",
    "    \n",
    "    num_terms_class_c_vocab_c = []\n",
    "    for c in range(num_of_classes):\n",
    "        summation=0\n",
    "        for term in global_vocab:\n",
    "            if(term in tf_icf_list_of_dict[c].keys()):\n",
    "                summation+=tf_list_of_dict[c][term]\n",
    "        num_terms_class_c_vocab_c.append(summation)\n",
    "    \n",
    "    # testing\n",
    "    confusion_matrix = np.zeros((num_of_classes,num_of_classes))\n",
    "    total_predictions = 0\n",
    "    for c in range(num_of_classes):\n",
    "        for test_doc_tokens in train_test_lists[c][1]:\n",
    "            posterior_class_c = []\n",
    "            for cc in range(num_of_classes):\n",
    "                posterior_class_c.append(prior_class_c[cc])\n",
    "            for term in test_doc_tokens:\n",
    "                for class_c in range(num_of_classes):\n",
    "                    if(term not in global_vocab):\n",
    "                        continue\n",
    "                    temp = 1.0\n",
    "                    if(term in list_of_vocab_class_c[class_c]):\n",
    "                        temp = (1 + tf_list_of_dict[class_c][term])/(len(global_vocab) + num_terms_class_c_vocab_c[class_c])\n",
    "                    else:\n",
    "                        temp = 1/(len(global_vocab) + num_terms_class_c_vocab_c[class_c])\n",
    "                    posterior_class_c[class_c] = posterior_class_c[class_c]*temp\n",
    "            predicted_class = posterior_class_c.index(max(posterior_class_c))\n",
    "            total_predictions+=1\n",
    "            confusion_matrix[c][predicted_class]+=1\n",
    "    print(confusion_matrix)\n",
    "    \n",
    "    return 100*np.trace(confusion_matrix)/total_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94fbc5b",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b3e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    random.seed(0)\n",
    "    num_of_classes = 5\n",
    "    train_fractions = [0.5, 0.7, 0.8]\n",
    "    \n",
    "    # create set of stop words for preprocessing\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    \n",
    "    # Get List of Files in Dataset\n",
    "    list_of_files_class_0 = getListOfFiles('Dataset/20_newsgroups/comp.graphics')\n",
    "    list_of_files_class_1 = getListOfFiles('Dataset/20_newsgroups/sci.med')\n",
    "    list_of_files_class_2 = getListOfFiles('Dataset/20_newsgroups/talk.politics.misc')\n",
    "    list_of_files_class_3 = getListOfFiles('Dataset/20_newsgroups/rec.sport.hockey')\n",
    "    list_of_files_class_4 = getListOfFiles('Dataset/20_newsgroups/sci.space')\n",
    "    list_of_files = [list_of_files_class_0, list_of_files_class_1, list_of_files_class_2, list_of_files_class_3, list_of_files_class_4]\n",
    "    \n",
    "    # Preprocess documents\n",
    "    #preprocessed_list_of_docs_tokens = preprocess_documents(list_of_files, num_of_classes, stopwords_set)\n",
    "    pi_file = open('Q3_pkl_file', 'rb')\n",
    "    preprocessed_list_of_docs_tokens = pickle.load(pi_file)\n",
    "    pi_file.close()\n",
    "    \n",
    "    top_k = int(input('Enter value of k: '))\n",
    "    print()\n",
    "    for train_frac in train_fractions:\n",
    "        train_test_lists = []\n",
    "        for class_num in range(num_of_classes):\n",
    "            train_list_class_c, test_list_class_c = train_test_split_function(train_frac, preprocessed_list_of_docs_tokens[class_num])\n",
    "            train_test_lists.append([train_list_class_c, test_list_class_c])\n",
    "        print()\n",
    "        accuracy = Q3(train_test_lists, num_of_classes, top_k)\n",
    "        print('Accuracy for train split fraction ',train_frac, ' is ',accuracy,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34e1da78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter value of k: 10\n",
      "\n",
      "\n",
      "[[498.   1.   0.   0.   1.]\n",
      " [  6. 494.   0.   0.   0.]\n",
      " [  4.  33. 462.   0.   1.]\n",
      " [  0.   0.   0. 500.   0.]\n",
      " [ 11.   0.   0.   0. 489.]]\n",
      "Accuracy for train split fraction  0.5  is  97.72 %\n",
      "\n",
      "[[299.   1.   0.   0.   0.]\n",
      " [  1. 299.   0.   0.   0.]\n",
      " [  2.  22. 275.   0.   1.]\n",
      " [  0.   0.   0. 300.   0.]\n",
      " [  6.   1.   0.   0. 293.]]\n",
      "Accuracy for train split fraction  0.7  is  97.73333333333333 %\n",
      "\n",
      "[[200.   0.   0.   0.   0.]\n",
      " [  2. 197.   0.   1.   0.]\n",
      " [  2.  17. 181.   0.   0.]\n",
      " [  0.   0.   0. 200.   0.]\n",
      " [  2.   1.   0.   0. 197.]]\n",
      "Accuracy for train split fraction  0.8  is  97.5 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
