{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd0fc50",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69716362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import heapq\n",
    "import operator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d184f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sortedcontainers import SortedDict, SortedList, SortedSet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f4841",
   "metadata": {},
   "source": [
    "# Read From Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5712b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(directory):\n",
    "    '''\n",
    "    Parameters:\n",
    "        directory: type(string)\n",
    "        \n",
    "    returns: list of all files in directory with the full path of file\n",
    "    '''\n",
    "    \n",
    "    list_of_files = []\n",
    "    \n",
    "    for file_path in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, file_path)\n",
    "        if os.path.isfile(full_path):\n",
    "            list_of_files.append(full_path)\n",
    "    \n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a7c84",
   "metadata": {},
   "source": [
    "# Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6127280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(data):\n",
    "    '''\n",
    "    Parameters:\n",
    "        data: type(string)\n",
    "    \n",
    "    returns: lowercase of data\n",
    "    '''\n",
    "    \n",
    "    return data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063ff0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_word_tokenize(corpus):\n",
    "    '''\n",
    "    Parameters:\n",
    "        corpus: type(string)\n",
    "    \n",
    "    returns word-level tokenization of corpus\n",
    "    '''\n",
    "    \n",
    "    return word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf64e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_tokens(tokens, stopwords_set):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "        stopwords_set: type(set)\n",
    "    \n",
    "    returns: tokens without stopwords\n",
    "    '''\n",
    "    tokens_sans_stopwords = [x for x in tokens if x not in stopwords_set]\n",
    "    \n",
    "    return tokens_sans_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a7cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_from_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: tokens without punctuation\n",
    "    '''\n",
    "    tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in tokens]\n",
    "    \n",
    "    return tokens_sans_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c367e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_blank_space_tokens(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    returns: tokens without blank tokens\n",
    "    '''\n",
    "    tokens_sans_blank_space = [x for x in tokens if x!='']\n",
    "    \n",
    "    return tokens_sans_blank_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ab3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens):\n",
    "    '''\n",
    "    Parameters:\n",
    "        tokens: type(list)\n",
    "    \n",
    "    retruns: tokens after stemming\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_tokens = [stemmer.stem(x) for x in tokens]\n",
    "    \n",
    "    return stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66582234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, stopwords_set):\n",
    "    # Convert the text to lower case\n",
    "    lowercase_corpus = lowercase(corpus)\n",
    "    #print(len(lowercase_corpus))\n",
    "    \n",
    "    # Perform word tokenization (word_tokenize also takes care of whitespace)\n",
    "    word_tokens = perform_word_tokenize(lowercase_corpus)\n",
    "    #print(len(word_tokens))\n",
    "    \n",
    "    # Remove stopwords from tokens\n",
    "    word_tokens_sans_stopwords = remove_stopwords_from_tokens(word_tokens, stopwords_set)\n",
    "    #print(len(word_tokens_sans_stopwords))\n",
    "    \n",
    "    # Remove punctuation marks from tokens\n",
    "    word_tokens_sans_punctuation = remove_punctuation_from_tokens(word_tokens_sans_stopwords)\n",
    "    #print(len(word_tokens_sans_punctuation))\n",
    "    \n",
    "    # Remove blank space tokens\n",
    "    word_tokens_sans_blank_tokens = remove_blank_space_tokens(word_tokens_sans_punctuation)\n",
    "    #print(len(word_tokens_sans_blank_tokens))\n",
    "    \n",
    "    # Stem tokens\n",
    "    #word_tokens_final = stemming(word_tokens_sans_blank_tokens)\n",
    "    \n",
    "    return word_tokens_sans_blank_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db7ed216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_documents(list_of_files, stopwords_set):\n",
    "    '''\n",
    "    Parameters:\n",
    "        list_of_files: type(list)\n",
    "        stopwords_set: type(set)\n",
    "    \n",
    "    returns: list of tokens obtained by preprocessing documents in all classes\n",
    "    '''\n",
    "    preprocessed_list_of_docs_tokens = []\n",
    "    for doc_path in list_of_files:\n",
    "        file = open(doc_path, 'r', encoding='utf-8', errors='ignore')\n",
    "        file_corpus = file.read()\n",
    "        file.close()\n",
    "        doc_tokens = preprocess(file_corpus, stopwords_set)\n",
    "        preprocessed_list_of_docs_tokens.append(doc_tokens)\n",
    "    \n",
    "    pi_file = open('Q1_tf_idf.pkl', 'wb')\n",
    "    pickle.dump(preprocessed_list_of_docs_tokens, pi_file)\n",
    "    pi_file.close()\n",
    "    \n",
    "    return preprocessed_list_of_docs_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda19dc7",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce10845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_dictionary(list_of_files):\n",
    "    '''\n",
    "    Paramteres:\n",
    "        list_of_files: type(string)\n",
    "    \n",
    "    returns: file_dictionary with integer key and path_of_file as value\n",
    "    '''\n",
    "    file_dictionary = {}\n",
    "    for i in range(len(list_of_files)):\n",
    "        file_dictionary[i] = list_of_files[i]\n",
    "    \n",
    "    return file_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c55d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2_way_mapping_of_term_and_doc(global_vocabulary):\n",
    "    '''\n",
    "    Parameter:\n",
    "        global_vocabulary: type(list)\n",
    "    \n",
    "    returns: 2 dictionaries with key-value pair as term:docID and docID:term respectively\n",
    "    '''\n",
    "    term_vs_ID_dict = {}\n",
    "    ID_vs_term_dict = {}\n",
    "    \n",
    "    for idx,term in enumerate(global_vocabulary):\n",
    "        ID_vs_term_dict[idx] = term\n",
    "        term_vs_ID_dict[term] = idx\n",
    "    \n",
    "    return term_vs_ID_dict, ID_vs_term_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7953f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_words_in_doc(TF_IDF_doc_vs_term_matrix):\n",
    "    return np.sum(TF_IDF_doc_vs_term_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7063a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_freq_term_in_doc(matrix):\n",
    "    return np.amax(matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6753a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_IDF(inverted_index, total_documents):\n",
    "    IDF = {}\n",
    "    for term in inverted_index:\n",
    "        IDF[term] = np.log10(total_documents/(1 + len(inverted_index[term])))\n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f32dff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Binary_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix):\n",
    "    '''\n",
    "    Parameters:\n",
    "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
    "    \n",
    "    returns: binary weighting scheme\n",
    "    '''\n",
    "    return np.where((TF_IDF_doc_vs_term_matrix <= 0),0,1).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25d3160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Raw_count_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix):\n",
    "    '''\n",
    "    Parameters:\n",
    "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
    "    \n",
    "    returns: raw count weighing scheme\n",
    "    '''\n",
    "    return TF_IDF_doc_vs_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d6d97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Term_frequency_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix, total_words_in_doc):\n",
    "    '''\n",
    "    Parameters:\n",
    "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
    "        total_words_in_doc: type(np.array)\n",
    "    \n",
    "    returns: term frequency weighing scheme\n",
    "    '''\n",
    "    return (TF_IDF_doc_vs_term_matrix.T / total_words_in_doc).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29df17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Log_Normalization_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix):\n",
    "    '''\n",
    "    Parameters:\n",
    "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
    "    \n",
    "    returns: log normalization weighing scheme\n",
    "    '''\n",
    "    return np.log10(1+TF_IDF_doc_vs_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0065a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Double_Normalization_weighing_TF_IDF(TF_IDF_doc_vs_term_matrix, max_freq_term_in_doc):\n",
    "    '''\n",
    "    Parameters:\n",
    "        TF_IDF_doc_vs_term_matrix: type(np.ndarray)\n",
    "    \n",
    "    returns: double normalization weighing scheme\n",
    "    '''\n",
    "    matrix = (TF_IDF_doc_vs_term_matrix.T / max_freq_term_in_doc).T\n",
    "\n",
    "    return 0.5 + 0.5*matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e491f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variants(matrix, total_terms, max_freq, size_of_global_vocabulary, IDF, ID_vs_term_dict, isQuery):\n",
    "    '''\n",
    "    returns: variants of TF-IDF\n",
    "    '''\n",
    "    Binary = Binary_weighing_TF_IDF(matrix)\n",
    "    Raw_count = Raw_count_weighing_TF_IDF(matrix)\n",
    "    Term_frequency = Term_frequency_weighing_TF_IDF(matrix, total_terms)\n",
    "    Log_Normalization = Log_Normalization_weighing_TF_IDF(matrix)\n",
    "    Double_Normalization = Double_Normalization_weighing_TF_IDF(matrix, max_freq)\n",
    "    \n",
    "    if(isQuery):\n",
    "        for i in tqdm(range(size_of_global_vocabulary)):\n",
    "            IDF_factor = IDF[ID_vs_term_dict[i]]\n",
    "            Binary[i] *= IDF_factor\n",
    "            Raw_count[i] *= IDF_factor\n",
    "            Term_frequency[i] *= IDF_factor\n",
    "            Log_Normalization[i] *= IDF_factor\n",
    "            Double_Normalization[i] *= IDF_factor\n",
    "    else:\n",
    "        for i in tqdm(range(size_of_global_vocabulary)):\n",
    "            IDF_factor = IDF[ID_vs_term_dict[i]]\n",
    "            Binary[:,i] *= IDF_factor\n",
    "            Raw_count[:,i] *= IDF_factor\n",
    "            Term_frequency[:,i] *= IDF_factor\n",
    "            Log_Normalization[:, i] *= IDF_factor\n",
    "            Double_Normalization[:, i] *= IDF_factor\n",
    "    \n",
    "    return Binary, Raw_count, Term_frequency, Log_Normalization, Double_Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d360230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(N, TF_IDF, query_of_same_variant, file_dictionary):\n",
    "    '''\n",
    "    evaluates and displays top N relevant documents based on score\n",
    "    '''\n",
    "    tf_idf_score = np.dot(TF_IDF, query_of_same_variant)\n",
    "    tf_idf_score = {file_dictionary[i]:tf_idf_score[i] for i in range(len(tf_idf_score))}\n",
    "    \n",
    "    relevant_docs = list(sorted(tf_idf_score.items(), key=operator.itemgetter(1),reverse=True))[:N]\n",
    "    for docs in relevant_docs:\n",
    "        print('Score: {}  Document: {}'.format(docs[1], docs[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e48448",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcb16338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # create set of stop words for preprocessing\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    \n",
    "    # Get List of Files in Dataset\n",
    "    list_of_files = getListOfFiles('Dataset/Humor,Hist,Media,Food/')\n",
    "    \n",
    "    # create dictionary of file with docID (integer) as key and full_path of file as value\n",
    "    file_dictionary = create_file_dictionary(list_of_files)\n",
    "    total_documents = len(file_dictionary)\n",
    "    \n",
    "    #preprocessed_list_of_docs_tokens = preprocess_documents(list_of_files, stopwords_set)\n",
    "    pi_file = open('Q1_tf_idf.pkl', 'rb')\n",
    "    preprocessed_list_of_docs_tokens = pickle.load(pi_file)\n",
    "    pi_file.close()\n",
    "    \n",
    "    # Global list of terms\n",
    "    global_list_of_terms = []\n",
    "    for doc in preprocessed_list_of_docs_tokens:\n",
    "        global_list_of_terms.extend(doc)\n",
    "    \n",
    "    # finding all distinct terms across all documents\n",
    "    global_vocabulary = list(set(global_list_of_terms))\n",
    "    size_of_global_vocabulary = len(global_vocabulary)\n",
    "    \n",
    "    # term vs docID 2 way mapping\n",
    "    term_vs_ID_dict, ID_vs_term_dict = create_2_way_mapping_of_term_and_doc(global_vocabulary)\n",
    "    \n",
    "    # term-doc matrix (for storing frequency of each word of global_vocabulary in doc)\n",
    "    inverted_index = {}\n",
    "    size_of_TF_IDF_matrix = (total_documents, size_of_global_vocabulary)\n",
    "    TF_IDF_doc_vs_term_matrix = np.zeros(size_of_TF_IDF_matrix, dtype=float)\n",
    "    \n",
    "    for i in tqdm(range(len(preprocessed_list_of_docs_tokens))):\n",
    "        for term in preprocessed_list_of_docs_tokens[i]:\n",
    "            if(term in inverted_index):\n",
    "                if(inverted_index[term][-1]!=i):\n",
    "                    inverted_index[term].append(i)\n",
    "            else:\n",
    "                inverted_index[term] = [i]\n",
    "            TF_IDF_doc_vs_term_matrix[i][term_vs_ID_dict[term]] += 1\n",
    "    \n",
    "    total_words_in_doc = get_total_words_in_doc(TF_IDF_doc_vs_term_matrix)\n",
    "    \n",
    "    max_freq_term_in_doc = get_max_freq_term_in_doc(TF_IDF_doc_vs_term_matrix)\n",
    "    \n",
    "    # Calculate IDF\n",
    "    IDF = calculate_IDF(inverted_index, total_documents)\n",
    "    \n",
    "    Binary_tf_idf, Raw_count_tf_idf, Term_frequency_tf_idf, Log_Normalization_tf_idf, Double_Normalization_tf_idf = compute_variants(TF_IDF_doc_vs_term_matrix, total_words_in_doc, max_freq_term_in_doc, size_of_global_vocabulary, IDF, ID_vs_term_dict, False)\n",
    "    \n",
    "    query = input(\"Input query: \")\n",
    "    sanitized_query = preprocess(query, stopwords_set)\n",
    "    print(\"Sanitized query: \", sanitized_query)\n",
    "\n",
    "    query_frequency = {}\n",
    "    for query_token in sanitized_query:\n",
    "        if query_token in query_frequency:\n",
    "            query_frequency[query_token]+=1\n",
    "        else:\n",
    "            query_frequency[query_token]=1\n",
    "    \n",
    "    # create query frequency vector\n",
    "    query_frequency_vector = np.zeros((size_of_global_vocabulary,1))\n",
    "    for token in query_frequency.keys():\n",
    "        if token in global_vocabulary:\n",
    "            query_frequency_vector[term_vs_ID_dict[token]] = query_frequency[token]\n",
    "    \n",
    "    \n",
    "    max_freq_token = query_frequency[max(query_frequency, key=query_frequency.get)]\n",
    "    query_binary, query_raw_count, query_term_frequency, query_log_normalization, query_double_normalization = compute_variants(query_frequency_vector, len(sanitized_query), max_freq_token, size_of_global_vocabulary, IDF, ID_vs_term_dict, True)\n",
    "    \n",
    "    print('Binary Scheme: Top 5 relevant documents are:')\n",
    "    topN(5, Binary_tf_idf, query_binary, file_dictionary)\n",
    "    \n",
    "    print('Raw Count Scheme: Top 5 relevant documents are:')\n",
    "    topN(5, Raw_count_tf_idf, query_raw_count, file_dictionary)\n",
    "    \n",
    "    print('Term Frequency Scheme: Top 5 relevant documents are:')\n",
    "    topN(5, Term_frequency_tf_idf, query_term_frequency, file_dictionary)\n",
    "    \n",
    "    print('Log Normalization Scheme: Top 5 relevant documents are:')\n",
    "    topN(5, Log_Normalization_tf_idf, query_log_normalization, file_dictionary)\n",
    "    \n",
    "    print('Double_Normalization Scheme: Top 5 relevant documents are:')\n",
    "    topN(5, Double_Normalization_tf_idf, query_double_normalization, file_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1b259c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1133/1133 [00:01<00:00, 746.62it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 82779/82779 [00:08<00:00, 10274.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input query: Demo taken successfully by students\n",
      "Sanitized query:  ['demo', 'taken', 'successfully', 'students']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 82779/82779 [00:01<00:00, 81052.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Scheme: Top 5 relevant documents are:\n",
      "Score: [5.41009336]  Document: Dataset/Humor,Hist,Media,Food/basehead.txt\n",
      "Score: [5.41009336]  Document: Dataset/Humor,Hist,Media,Food/comic_st.gui\n",
      "Score: [5.41009336]  Document: Dataset/Humor,Hist,Media,Food/hackingcracking.txt\n",
      "Score: [5.41009336]  Document: Dataset/Humor,Hist,Media,Food/vegan.rcp\n",
      "Score: [4.88026362]  Document: Dataset/Humor,Hist,Media,Food/cooplaws\n",
      "\n",
      "Raw Count Scheme: Top 5 relevant documents are:\n",
      "Score: [49.33246593]  Document: Dataset/Humor,Hist,Media,Food/basehead.txt\n",
      "Score: [25.47308373]  Document: Dataset/Humor,Hist,Media,Food/jason.fun\n",
      "Score: [16.44535009]  Document: Dataset/Humor,Hist,Media,Food/practica.txt\n",
      "Score: [14.21094867]  Document: Dataset/Humor,Hist,Media,Food/humor9.txt\n",
      "Score: [11.53857608]  Document: Dataset/Humor,Hist,Media,Food/bw.txt\n",
      "\n",
      "Term Frequency Scheme: Top 5 relevant documents are:\n",
      "Score: [0.00649293]  Document: Dataset/Humor,Hist,Media,Food/liceprof.sty\n",
      "Score: [0.00612377]  Document: Dataset/Humor,Hist,Media,Food/grammar.jok\n",
      "Score: [0.00472894]  Document: Dataset/Humor,Hist,Media,Food/cooplaws\n",
      "Score: [0.00448776]  Document: Dataset/Humor,Hist,Media,Food/languag.jok\n",
      "Score: [0.00414545]  Document: Dataset/Humor,Hist,Media,Food/butstcod.fis\n",
      "\n",
      "Log Normalization Scheme: Top 5 relevant documents are:\n",
      "Score: [1.57792864]  Document: Dataset/Humor,Hist,Media,Food/basehead.txt\n",
      "Score: [0.69203106]  Document: Dataset/Humor,Hist,Media,Food/humor9.txt\n",
      "Score: [0.61937452]  Document: Dataset/Humor,Hist,Media,Food/bw.txt\n",
      "Score: [0.56635585]  Document: Dataset/Humor,Hist,Media,Food/hackingcracking.txt\n",
      "Score: [0.5619592]  Document: Dataset/Humor,Hist,Media,Food/jason.fun\n",
      "\n",
      "Double_Normalization Scheme: Top 5 relevant documents are:\n",
      "Score: [133741.89004051]  Document: Dataset/Humor,Hist,Media,Food/curse.txt\n",
      "Score: [133717.15874731]  Document: Dataset/Humor,Hist,Media,Food/a_tv_t-p.com\n",
      "Score: [133715.28346495]  Document: Dataset/Humor,Hist,Media,Food/strine.txt\n",
      "Score: [133706.30332188]  Document: Dataset/Humor,Hist,Media,Food/growth.txt\n",
      "Score: [133705.29972214]  Document: Dataset/Humor,Hist,Media,Food/turbo.hum\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
